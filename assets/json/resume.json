{
  "basics": {
    "name": "Ruiyang Zhou",
    "label": "PhD Student",
    "image": "",
    "email": "ruiyang.zhou@utexas.edu",
    "phone": "",
    "url": "",
    "summary": "First year PhD student in UT Austin, under the supervision of Prof. Leqi Liu.",
    "location": {
    },
    "profiles": [
      {
      }
    ]
  },
  "work": [
  
  ],
  "volunteer": [
  
  ],
  "education": [
    {
      "university": "University of Texas at Austin",
      "department": "IROM - McCombs School of Business",
      "laboratory": "HUMAIN Lab",
      "url": "https://leqiliu.github.io/humain_lab",
      "area": "Use-Inspired AI",
      "studyType": "PhD student",
      "startDate": "2024-09-01"
    },
    {
      "university": "Shanghai Jiao Tong University",
      "department": "SPEIT",
      "laboratory": "XLance Lab",
      "url": "https://x-lance.github.io/",
      "area": "Natural Language Processing",
      "studyType": "Master",
      "startDate": "2021-09-01",
      "endDate": "2024-03-01"
    },
    {
      "university": "Shanghai Jiao Tong University",
      "department": "SPEIT",
      "laboratory": "XLance Lab",
      "url": "https://x-lance.github.io/",
      "area": "Natural Language Processing",
      "studyType": "Master",
      "startDate": "2021-09-01",
      "endDate": "2024-03-01"
    },
    {
      "university": "Shanghai Jiao Tong University",
      "department": "SPEIT",
      "laboratory": "XLance Lab",
      "url": "https://x-lance.github.io/",
      "area": "Natural Language Processing",
      "studyType": "Master",
      "startDate": "2021-09-01",
      "endDate": "2024-03-01"
    }
  ],
  "awards": [
   
  ],
  "certificates": [
    
  ],
  "publications": [
    {
      "name": "Is LLM a Reliable Reviewer? A Comprehensive Evaluation of LLM on Automatic Paper Reviewing Tasks",
      "publisher": "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
      "releaseDate": "May 2024",
      "url": "https://aclanthology.org/2024.lrec-main.816/",
      "summary": "The use of large language models (LLM), especially ChatGPT, to help with research has come into practice. Researchers use it for timely advice and hope to obtain in-depth feedback. However, can LLM be a qualified and reliable reviewer? Although there already exist several review-related datasets, few works have carefully and thoroughly inspected modelâ€™s capability as a reviewer, especially the correctness of generated reviews. In this paper, we first evaluate GPT-3.5 and GPT-4 (the current top-performing LLM) on 2 types of tasks under different settings: the score prediction task and the review generation task. In addition, we propose a dataset containing 197 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in ICLR-2023. By asking questions from technical details to the overall presentation and quality, our RR-MCQ data provides a more complete model ability assessment. The results show that LLM is generally helpful, but great caution is needed as it always makes mistakes. Although it can give passable decisions (> 60% accuracy) on single options, completely correct answers are still rare (about 20%); models are still weak on long paper processing, zero-shot scoring, and giving critical feedback like human reviewers."
    }
  ],
  "skills": [
    
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Fluent",
      "icon": ""
    },
    {
      "language": "French",
      "fluency": "Fluent",
      "icon": ""
    },
    {
      "language": "Chinese",
      "fluency": "Native",
      "icon": ""
    }
  ],

  "projects": [
    {
      "name": "PRLHF",
      "summary": "prlhf",
      "highlights": ["prlhf"],
      "startDate": "2018-01-01",
      "endDate": "2018-01-01",
      "url": "https://example.com"
    }
  ]
}
